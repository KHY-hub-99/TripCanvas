import requests
from urllib.parse import urlencode, unquote
import xml.etree.ElementTree as ET
import pandas as pd
import time
import math
import sys


sys.stdout.reconfigure(encoding='utf-8')

page_no = 1
key = "d76ac5e297694f1394b83e27c3db86757fa9841547096b05c70788028361e5a1"
base_url = f"http://apis.data.go.kr/B551011/KorService2/areaBasedList2"    

def fetch_page_data(page_num, num_of_rows, contentTypeId):
    """
    ID  ì½˜í…ì¸               ì„¤ëª…
    12	ê´€ê´‘ì§€	            ê°€ì¥ ì¼ë°˜ì ì¸ ê´€ê´‘ ëª…ì†Œ (ê¶ê¶, ì‚¬ì°°, ê³µì›, ë°•ë¬¼ê´€, ìì—° ëª…ì†Œ ë“±)
    14	ë¬¸í™”ì‹œì„¤	        ë°•ë¬¼ê´€, ë¯¸ìˆ ê´€, ê³µì—°ì¥, ê¸°ë…ê´€ ë“± ë¬¸í™” í™œë™ ê´€ë ¨ ì‹œì„¤
    15	í–‰ì‚¬/ê³µì—°/ì¶•ì œ	     ê¸°ê°„ì´ í•œì •ëœ ì´ë²¤íŠ¸ ì •ë³´ (ì§€ì—­ ì¶•ì œ, ì½˜ì„œíŠ¸, ì •ê¸° ê³µì—° ë“±)
    25	ì—¬í–‰ ì½”ìŠ¤	        ì—¬ëŸ¬ ì¥ì†Œë¥¼ ë¬¶ì–´ ì œê³µí•˜ëŠ” ì¶”ì²œ ì—¬í–‰ ê²½ë¡œ (ë„ë³´, ìì „ê±° ì½”ìŠ¤ ë“±)
    28	ë ˆí¬ì¸ 	            ë“±ì‚°, ìŠ¤í‚¤, ê³¨í”„, ìˆ˜ìƒ ë ˆí¬ì¸  ë“± ìŠ¤í¬ì¸  ë° ì—¬ê°€ í™œë™ ì‹œì„¤
    32	ìˆ™ë°•	            í˜¸í…”, ì½˜ë„, íœì…˜, ê²ŒìŠ¤íŠ¸í•˜ìš°ìŠ¤, í•œì˜¥ ë“± ìˆ™ë°• ì‹œì„¤
    38	ì‡¼í•‘	            ì‹œì¥, ë°±í™”ì , ë©´ì„¸ì , ì „ë¬¸ ì‡¼í•‘ëª° ë“± êµ¬ë§¤ ê´€ë ¨ ì‹œì„¤
    39	ìŒì‹ì               ë§›ì§‘, ì „ë¬¸ ì‹ë‹¹ ë“± ì‹ë„ë½ ê´€ë ¨ ì‹œì„¤
    
    
    1	ì„œìš¸íŠ¹ë³„ì‹œ
    2	ì¸ì²œê´‘ì—­ì‹œ
    3	ëŒ€ì „ê´‘ì—­ì‹œ
    4	ëŒ€êµ¬ê´‘ì—­ì‹œ
    5	ê´‘ì£¼ê´‘ì—­ì‹œ
    6	ë¶€ì‚°ê´‘ì—­ì‹œ
    7	ìš¸ì‚°ê´‘ì—­ì‹œ
    8	ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ
    31	ê²½ê¸°ë„
    32	ê°•ì›íŠ¹ë³„ìì¹˜ë„
    33	ì¶©ì²­ë¶ë„
    34	ì¶©ì²­ë‚¨ë„
    35	ê²½ìƒë¶ë„
    36	ê²½ìƒë‚¨ë„
    37	ì „ë¶íŠ¹ë³„ìì¹˜ë„
    38	ì „ë¼ë‚¨ë„
    39	ì œì£¼íŠ¹ë³„ìì¹˜ë„
    """
    try:
        params = {
            'numOfRows': num_of_rows,
            'pageNo': page_num,
            'MobileOS': 'ETC',
            'MobileApp': 'AppTest',
            'ServiceKey': unquote(key), # Use unquote if the key might contain URL-encoded chars
            'arrange': 'A',
            'contentTypeId': contentTypeId, # 12 usually means 'Attractions' or 'Tour Sites'
            'areaCode': '',
            'sigunguCode': '',
            'cat1': '',
            'cat2': '',
            'cat3': ''
        }

        # Encode parameters for the final URL
        query_string = urlencode(params)
        request_url = f"{base_url}?{query_string}"


        response = requests.get(request_url, timeout=300) # Added a timeout
        time.sleep(5)
        response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)

        root = ET.fromstring(response.content)
        items = root.findall('./body/items/item')
        
        if not items:
                print(f"âš ï¸ í˜ì´ì§€ {page_no}ì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ë°ì´í„° ì†Œì§„ ë˜ëŠ” ì˜¤ë¥˜)")

        # Simple parsing logic (you may need to extend this)
        page_data = []
        for item in items:
            data = {}
            # Example fields - retrieve fields relevant to your need
            data['title'] = item.find('title').text if item.find('title') is not None else 'N/A'
            data['area'] = item.find('areacode').text if item.find('areacode') is not None else 'N/A'
            data['contentid'] = item.find('contentid').text if item.find('contentid') is not None else 'N/A'
            addr1 = item.find('addr1').text if item.find('addr1') is not None else ''
            addr2 = item.find('addr2').text if item.find('addr2') is not None else ''
            address_parts = [addr1, addr2]
            data['addr'] = ' '.join(filter(None, address_parts)) or 'N/A'
            data["cat"] = item.find('cat1').text if item.find('cat1') is not None else 'N/A'
            data['x'] = item.find('mapx').text if item.find('mapx') is not None else "N/A"
            data['y'] = item.find('mapy').text if item.find('mapy') is not None else "N/A"
            page_data.append(data)
                
        return page_data

    except requests.exceptions.HTTPError as e:
        print(f"ğŸš¨ HTTP Error fetching page {page_num}: {e}")
        print(f"URL: {request_url}")
        return []
    except requests.exceptions.RequestException as e:
        print(f"ğŸš¨ Network/Connection Error fetching page {page_num}: {e}")
        return []
    except ET.ParseError:
        print(f"ğŸš¨ XML Parse Error for page {page_num}. Content: {response.text[:200]}...") # ì—ëŸ¬ ë‚´ìš© ì¼ë¶€ ì¶œë ¥
        return []
    except Exception as e:
        print(f"ğŸš¨ An unexpected error occurred for page {page_num}: {e}")
        return []

# total_count = 12894, rows_per_page = 12
def fetch_all_data(total_count, rows_per_page, contentTypeId):
    """
    ëª¨ë“  í˜ì´ì§€ë¥¼ ë°˜ë³µí•˜ë©° ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë©”ì¸ í•¨ìˆ˜
    """
    total_pages = math.ceil(total_count / rows_per_page)
    all_data = []
    
    print(f"âœ¨ ì´ {total_count}ê±´ì˜ ë°ì´í„°, í˜ì´ì§€ë‹¹ {rows_per_page}ê±´, ì´ {total_pages} í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.")
    
    for page_num in range(1, total_pages + 1):
        print(f"--- ğŸŒ í˜„ì¬ í˜ì´ì§€: {page_num}/{total_pages} ---")
        page_data = fetch_page_data(page_num, rows_per_page, contentTypeId)
        all_data.extend(page_data)
        
        # API ì„œë²„ì— ê³¼ë¶€í•˜ë¥¼ ì£¼ì§€ ì•Šê¸° ìœ„í•´ ì ì‹œ ëŒ€ê¸° (ì„ íƒ ì‚¬í•­)
        time.sleep(2) 
        
    print(f"âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ. ì´ {len(all_data)}ê±´ì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
    return all_data

def save_to_dataframe_and_csv(data_list, filename="tour_data.csv"):
    """
    ìˆ˜ì§‘ëœ ë¦¬ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ê³  CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    print("--- ğŸ’¾ ë°ì´í„° ë³€í™˜ ë° ì €ì¥ ì‹œì‘ ---")
    
    # 1. DataFrame ë³€í™˜
    df = pd.DataFrame(data_list)
    
    # 3. CSV íŒŒì¼ ì €ì¥
    # encoding='utf-8-sig'ëŠ” í•œê¸€ ê¹¨ì§ ë° ì—‘ì…€ì—ì„œ íŒŒì¼ ì—´ëŒ ì‹œ í˜¸í™˜ì„±ì„ ë†’ì—¬ì¤ë‹ˆë‹¤.
    df.to_csv(filename, index=False, encoding='utf-8-sig')
    
    print(f"âœ… ë°ì´í„°í”„ë ˆì„ ë³€í™˜ ì„±ê³µ.")
    print(f"âœ… {filename} íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ. (ì´ {len(df)} í–‰)")
    print(df.head())
    
    return df

# ìŒì‹ì  total 14126, ìˆ™ë°• total 3590
TOTAL_COUNT = 8206
ROWS_PER_PAGE = 1000
CONTENT_TYPE_ID = 38

# 1. API í˜¸ì¶œë¡œ ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘
all_data_list = fetch_all_data(TOTAL_COUNT, ROWS_PER_PAGE, CONTENT_TYPE_ID)

# 2. ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ê³  CSVë¡œ ì €ì¥
if all_data_list:
    final_df = save_to_dataframe_and_csv(all_data_list, filename="korea_tour_hotel.csv")
else:
    print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ì–´ DataFrame ë³€í™˜ ë° ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    final_df = pd.DataFrame()
